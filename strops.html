<!DOCTYPE HTML>
<html>
	<head>
		<title>strops</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<a href="index.html" class="back-arrow" aria-label="Back to Home">&#8592;</a>
					<h1>strops</h1>
					<p>a data engineering and analytics project for tracking statistics across various fantasy sports platforms hosted on PostgreSQL and Tableau Public.</p>
				</header>

				<!-- Main -->
					<div id="main">

						<!-- Content -->
						<section id="Introduction" class="main">
							<span class="image main"><img src="images/bg2.jpg" alt="" /></span>
							<h2>Introduction</h2>
							<p>Growing up, I loved watching sports such as baseball, basketball, and hockey, and always cheered for my favourite athletes Jeremy Lin and José Bautista. Over time, I developed a deep appreciation for the metrics and statistics in professional sports leagues. As I invested interest in more and more sports, the process of scrolling, swiping, and clicking on several platforms to track the statistics of teams and athletes became more and more tedious. </p>
						</section>
						<section id="Objective" class="main">
							<h2>Objective</h2>
							<p>Create a centralized application and visualization tool for fantasy sports with Python and Tableau. My tool will retrieve statistics of specific, chosen sports teams and athletes across any professional sports leagues such as NHL, NFL, NBA, MLB. Ultimately, my tool will enable simplified and personalized sports tracking.</p>
						</section>
						<section id="Approach" class="main">
							<h2>Approach</h2>
							<ol>
								<li><b>Data Collection and Storage</b></li>
								<p>In this initial phase, the focus will be on researching and identifying reliable data sources for sports statistics. A robust data collection and storage mechanism will be developed using Python, ensuring accuracy and consistency of the data. This phase will involve careful selection of sources to ensure the data quality aligns with the project’s goals.</p>
								
								<li><b>User Interface and Customization</b></li>
								<p>The project will shift its focus to designing a user-friendly and intuitive user interface. This phase involves implementing user customization features, such as the ability to select favorite teams, sports, and statistics. A responsive layout will be developed to ensure compatibility across various devices, offering a seamless experience for users on desktop, tablet, and mobile devices.</p>
						
								<li><b>Data Visualization and Exploration</b></li>
								<p>Next, I will be dedicated to creating interactive visualizations and dashboards using Tableau. This phase aims to empower users with the ability to explore data through filtering, sorting, and aggregation. Visual aids like charts, graphs, and maps will be incorporated to enhance data comprehension and provide clear, actionable insights to the users.</p>
							</ol>
							<p>By leveraging Python and Tableau, the tool will provide sports enthusiasts with a personalized and convenient way to track their favorite teams and athletes across multiple professional sports leagues. The final solution will seamlessly integrate data collection, user customization, and dynamic visualizations to create a powerful tool for sports fans.</p>						
						</section>
						<section id="Challenges and Lessons Learned" class="main">
							<h2>Challenges and Lessons Learned</h2>
							<p><b>Challenge #1: Selecting the right storage medium</b></p>
							<p>Strops was a personal project; I wanted to challenge myself and use a new platform to learn about databasing techniques. I used PostgreSQL, a free open-source platform. However, other options included MS-SQL, MongoDB and other non-relational databases, cloud providers like GCP and AWS, but as a start, I wanted to learn about maintaining a relational database.</p>
							<p><b>Challenge #2: Ensuring data has been verified.</b></p>
							<p>Thirdly, the veracity of data was in question as well as there were many websites and APIs that I could have retrieved my data from. For example, I could pull from APIs provided by the leagues, web-scrape using the Beautiful Soup library, or even tap in to existing sports databases. However, how would I know that the statistics match up across all platforms or if they were even correct? </p>
						</section>
						<section id="Outcomes and Next Steps" class="main">
							<h2>Outcomes and Next Steps</h2>
							<ol>
								<li><b>Streaming Processes with Kafka</b></li>
								<p>The next step involves integrating real-time data streaming processes using Apache Kafka. This will allow for the continuous ingestion and processing of live data, ensuring that the analysis remains up to date and responsive to real-time changes, such as live sports statistics or other real-time metrics.</p>
								<li><b>Re-Factoring Scripts for Cloud Integration</b></li>
								<p>I will also focus on refactoring the existing scripts to be cloud-compatible, optimizing them for scalable cloud environments like AWS or GCP. This will involve adjusting the architecture for distributed processing and storage, allowing the system to efficiently handle increased data volume and provide more flexible, scalable solutions.</p>
								<li><b>Implement Advanced Analytics & Machine Learning</b></li>
								<p>Integrating machine learning models for predictive analytics can offer deeper insights. For example, predicting future trends based on historical data (e.g., predicting player performance or game outcomes), or identifying hidden patterns in the data through unsupervised learning methods, could add significant value to the analysis.</p>
								<li><b>Create an Automated Reporting System</b></li>
								<p>Set up an automated system that generates and sends out reports based on data analysis. This could be helpful for stakeholders or users to receive regular insights without manual intervention. Incorporating natural language generation (NLG) could create human-readable summaries of insights, making the information more accessible to non-technical users.</p>
							</ol>
						</section>
					</div>
			</div>
			
			<div style="height: 4rem;"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>